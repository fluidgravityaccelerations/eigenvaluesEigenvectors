{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w9QCN85uKMgN",
    "outputId": "76c29afc-cc30-4bd7-f28f-1d7ae563cb94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pycuda in ./.local/lib/python3.10/site-packages (2025.1.1)\n",
      "Requirement already satisfied: platformdirs>=2.2.0 in /usr/lib/python3/dist-packages (from pycuda) (2.5.1)\n",
      "Requirement already satisfied: mako in ./.local/lib/python3.10/site-packages (from pycuda) (1.3.10)\n",
      "Requirement already satisfied: pytools>=2011.2 in ./.local/lib/python3.10/site-packages (from pycuda) (2025.2.1)\n",
      "Requirement already satisfied: siphash24>=1.6 in ./.local/lib/python3.10/site-packages (from pytools>=2011.2->pycuda) (1.7)\n",
      "Requirement already satisfied: typing-extensions>=4.5 in /usr/lib/python3/dist-packages (from pytools>=2011.2->pycuda) (4.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/lib/python3/dist-packages (from mako->pycuda) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pycuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JaUb5BNWEQ1o"
   },
   "source": [
    "**ASSUMPTION: Nrows <= Ncols**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aGhhD_uVgk8S"
   },
   "source": [
    "**ITERATIONS ARE NOT POSSIBLE DUE TO THE CACHING MECHANISM OF PYCUDA AND THE TEMPLATE COMPILING NEEDS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CHWd4FpeJW_u"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pycuda.autoinit\n",
    "import pycuda.driver as drv\n",
    "from pycuda.compiler import SourceModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9oEgmGjAE7f1"
   },
   "outputs": [],
   "source": [
    "#numMatrices       = np.uint32(1048576)\n",
    "#numMatrices       = np.uint32(5)\n",
    "#numMatrices       = np.uint32(4)\n",
    "numMatrices       = np.uint32(32*2*2*2*2*2*2*2*2*2*2*2*2*2*2*2*2*2*2)\n",
    "Nrows             = np.uint32(4)\n",
    "Ncols             = np.uint32(4)\n",
    "maxIter           = np.uint32(Nrows)  # Bidiagonalization iterations\n",
    "\n",
    "if (Nrows < Ncols):\n",
    "  LEN = Nrows\n",
    "else:\n",
    "  LEN = Nrows - 1\n",
    "\n",
    "workType          = np.float64\n",
    "#workType          = np.float32\n",
    "if workType       == np.float32:\n",
    "    dtype_str     = \"float\"\n",
    "    sturmTol      = np.float32(1e-7)\n",
    "    #sturmTol      = np.float32(1e-6)\n",
    "else:\n",
    "    dtype_str     = \"double\"\n",
    "    sturmTol      = np.float64(1e-13)\n",
    "    #sturmTol      = np.float64(1e-12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WtC4hlEMKE_H"
   },
   "source": [
    "Rearrange - Original storage of the matrices is columnwise, while output storage is interlaced rowwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "YedheE2IC2tz"
   },
   "outputs": [],
   "source": [
    "def makeRearrangeKernel(dtype_str, numMatrices, Nrows, Ncols):\n",
    "\n",
    "    return f\"\"\"\n",
    "    #define T {dtype_str}\n",
    "    #define numMatrices {numMatrices}\n",
    "    #define Nrows {Nrows}\n",
    "    #define Ncols {Ncols}\n",
    "\n",
    "    extern \"C\" {{\n",
    "    __global__ void rearrangeKernel(const T * __restrict__ inputMatrices, T * __restrict__ outputMatrices) {{\n",
    "        int tid = threadIdx.x + blockDim.x * blockIdx.x;\n",
    "        if (tid < numMatrices) {{\n",
    "            #pragma unroll\n",
    "            for (unsigned int i = 0; i < Nrows; i++)\n",
    "                #pragma unroll\n",
    "                for (unsigned int j = 0; j < Ncols; j++)\n",
    "                    outputMatrices[ tid + j * numMatrices + i * (numMatrices * Ncols) ]\n",
    "                      = inputMatrices[ tid * (Nrows * Ncols) + j * Nrows + i ];\n",
    "        }}\n",
    "    }}\n",
    "    }}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQG31O4IKT0N"
   },
   "source": [
    "Bidiagonalize - Assumes interlaced storage of the matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "iGQP-K8SKc-P"
   },
   "outputs": [],
   "source": [
    "def makeBidiagonalizeKernel(dtype_str, numMatrices, Nrows, Ncols):\n",
    "    \"\"\"\n",
    "    Restituisce la stringa CUDA per il kernel di bidiagonalizzazione,\n",
    "    assumendo che l’array d’ingresso sia già “rearrangiato” in formato\n",
    "    row-major interlacciato tra batch (output di rearrange).\n",
    "\n",
    "    - dtype_str: \"float\" o \"double\"\n",
    "    - numMatrices, Nrows, Ncols: costanti a compile-time\n",
    "    \"\"\"\n",
    "    return f\"\"\"\n",
    "    #define T {dtype_str}\n",
    "    #define numMatrices {numMatrices}\n",
    "    #define Nrows {Nrows}\n",
    "    #define Ncols {Ncols}\n",
    "\n",
    "    // ===============================================\n",
    "    // COMPUTE LEFT HOUSEHOLDER VECTORS (device function)\n",
    "    // ===============================================\n",
    "    __device__ __forceinline__ void computeLeftHouseholderVectors(\n",
    "        const T * __restrict__ inputMatrices,\n",
    "        T * __restrict__ outputLeftHouseholderVectors,\n",
    "        T &betaq,\n",
    "        const unsigned int tid,\n",
    "        const unsigned int iterCounter)\n",
    "    {{\n",
    "        T x0, norm2 = static_cast<T>(0), mu, v0;\n",
    "\n",
    "        #pragma unroll\n",
    "        for (unsigned int i = 0; i < iterCounter; i++)\n",
    "            outputLeftHouseholderVectors[i] = static_cast<T>(0);\n",
    "\n",
    "        // Leggiamo, per i = iterCounter+1 ... Nrows-1, l’elemento\n",
    "        // (row = i, col = iterCounter, batch = tid) nel layout interlacciato:\n",
    "        #pragma unroll\n",
    "        for (unsigned int i = iterCounter + 1; i < Nrows; i++)\n",
    "        {{\n",
    "            // offset interlacciato:\n",
    "            //    blocco di righe:    i * (numMatrices * Ncols)\n",
    "            //    blocco di colonne:  iterCounter * numMatrices\n",
    "            //    offset batch:       + tid\n",
    "            const unsigned int idx = i * (numMatrices * Ncols)\n",
    "                                   + iterCounter * numMatrices\n",
    "                                   + tid;\n",
    "            const T buffer = inputMatrices[idx];\n",
    "            outputLeftHouseholderVectors[i] = buffer;\n",
    "            norm2 += buffer * buffer;\n",
    "        }}\n",
    "\n",
    "        // x0 = elemento (row = iterCounter, col = iterCounter, batch = tid):\n",
    "        unsigned int idx_x0 = iterCounter * (numMatrices * Ncols)\n",
    "                            + iterCounter * numMatrices\n",
    "                            + tid;\n",
    "        x0 = inputMatrices[idx_x0];\n",
    "\n",
    "        if (norm2 == static_cast<T>(0))\n",
    "        {{\n",
    "            betaq = static_cast<T>(0);\n",
    "        }}\n",
    "        else\n",
    "        {{\n",
    "            mu = sqrt(x0 * x0 + norm2);\n",
    "            if (x0 <= static_cast<T>(0))\n",
    "                v0 = x0 - mu;\n",
    "            else\n",
    "                v0 = -norm2 / (x0 + mu);\n",
    "\n",
    "            T temp = static_cast<T>(1) / v0;\n",
    "            betaq = static_cast<T>(2) * ((v0 * v0) / (norm2 + (v0 * v0)));\n",
    "            outputLeftHouseholderVectors[iterCounter] = static_cast<T>(1);\n",
    "\n",
    "            #pragma unroll\n",
    "            for (unsigned int s = iterCounter + 1; s < Nrows; s++)\n",
    "                outputLeftHouseholderVectors[s] *= temp;\n",
    "        }}\n",
    "    }}\n",
    "\n",
    "    // ================================================\n",
    "    // COMPUTE RIGHT HOUSEHOLDER VECTORS (device function)\n",
    "    // ================================================\n",
    "    __device__ __forceinline__ void computeRightHouseholderVectors(\n",
    "        const T * __restrict__ inputVectors,\n",
    "        T * __restrict__ outputRightHouseholderVectors,\n",
    "        T &betap,\n",
    "        const unsigned int tid,\n",
    "        const unsigned int iterCounter)\n",
    "    {{\n",
    "        T x0, norm2 = static_cast<T>(0), mu, v0;\n",
    "\n",
    "        #pragma unroll\n",
    "        for (unsigned int i = 0; i < iterCounter + 1; i++)\n",
    "            outputRightHouseholderVectors[i] = static_cast<T>(0);\n",
    "\n",
    "        #pragma unroll\n",
    "        for (unsigned int i = iterCounter + 2; i < Ncols; i++)\n",
    "        {{\n",
    "            const T buffer = inputVectors[i];\n",
    "            outputRightHouseholderVectors[i] = buffer;\n",
    "            norm2 += buffer * buffer;\n",
    "        }}\n",
    "\n",
    "        x0 = inputVectors[iterCounter + 1];\n",
    "\n",
    "        if (norm2 == static_cast<T>(0))\n",
    "        {{\n",
    "            betap = static_cast<T>(0);\n",
    "        }}\n",
    "        else\n",
    "        {{\n",
    "            mu = sqrt(x0 * x0 + norm2);\n",
    "            if (x0 <= static_cast<T>(0))\n",
    "                v0 = x0 - mu;\n",
    "            else\n",
    "                v0 = -norm2 / (x0 + mu);\n",
    "\n",
    "            T temp = static_cast<T>(1) / v0;\n",
    "            betap = static_cast<T>(2) * ((v0 * v0) / (norm2 + v0 * v0));\n",
    "            outputRightHouseholderVectors[iterCounter + 1] = static_cast<T>(1);\n",
    "\n",
    "            #pragma unroll\n",
    "            for (unsigned int s = iterCounter + 2; s < Ncols; s++)\n",
    "                outputRightHouseholderVectors[s] *= temp;\n",
    "        }}\n",
    "    }}\n",
    "\n",
    "    extern \"C\" {{\n",
    "\n",
    "    // ===============================================\n",
    "    // GLOBAL KERNEL: bidiagonalizeKernel\n",
    "    // ===============================================\n",
    "    __global__ void bidiagonalizeKernel(\n",
    "        T *inputMatrices,           // batch di matrici, interlacciate row-major\n",
    "        const unsigned int maxIter) // massimo numero di iterazioni\n",
    "    {{\n",
    "        int tid = threadIdx.x + blockDim.x * blockIdx.x;\n",
    "        if (tid >= numMatrices) return;\n",
    "\n",
    "        // Buffer temporanei per ciascun thread:\n",
    "        T leftHouseholderVectors[Nrows];\n",
    "        T rightHouseholderVectors[Ncols];\n",
    "        T inputVectorsForRight[Ncols];\n",
    "        T wi[Nrows];\n",
    "        T xi[Ncols];\n",
    "        T zi[Ncols];\n",
    "        T betau = static_cast<T>(0), betav = static_cast<T>(0);\n",
    "\n",
    "        // Iterazioni:\n",
    "        for (unsigned int iterCounter = 0; iterCounter < maxIter; iterCounter++)\n",
    "        {{\n",
    "            if (iterCounter < Ncols - 2)\n",
    "            {{\n",
    "                // 1) Compute left Householder\n",
    "                computeLeftHouseholderVectors(\n",
    "                    inputMatrices,\n",
    "                    leftHouseholderVectors,\n",
    "                    betau,\n",
    "                    tid,\n",
    "                    iterCounter);\n",
    "\n",
    "                // 2) Build inputVectorsForRight (per riflessioni right)\n",
    "                #pragma unroll\n",
    "                for (unsigned int j = 0; j < Ncols; j++)\n",
    "                {{\n",
    "                    T sum = static_cast<T>(0);\n",
    "                    #pragma unroll\n",
    "                    for (unsigned int i = 0; i < Nrows; i++)\n",
    "                    {{\n",
    "                        // leggo elemento (row=i, col=j) della matrice \"tid\" nel layout interlacciato:\n",
    "                        unsigned int idx_val = i * (numMatrices * Ncols)\n",
    "                                             + j * numMatrices\n",
    "                                             + tid;\n",
    "                        T val = inputMatrices[idx_val];\n",
    "                        sum += -betau * leftHouseholderVectors[i] * val;\n",
    "                    }}\n",
    "                    // aggiungo l’elemento diagonale (row=iterCounter, col=j) di A^(tid):\n",
    "                    unsigned int idx_diag = iterCounter * (numMatrices * Ncols)\n",
    "                                          + j * numMatrices\n",
    "                                          + tid;\n",
    "                    T diag = inputMatrices[idx_diag];\n",
    "                    inputVectorsForRight[j] = sum + diag;\n",
    "                    xi[j] = -sum;\n",
    "                }}\n",
    "\n",
    "                // 3) Compute right Householder\n",
    "                computeRightHouseholderVectors(\n",
    "                    inputVectorsForRight,\n",
    "                    rightHouseholderVectors,\n",
    "                    betav,\n",
    "                    tid,\n",
    "                    iterCounter);\n",
    "\n",
    "                // 4) Build wi\n",
    "                #pragma unroll\n",
    "                for (unsigned int i = 0; i < Nrows; i++)\n",
    "                {{\n",
    "                    T sum = static_cast<T>(0);\n",
    "                    #pragma unroll\n",
    "                    for (unsigned int j = 0; j < Ncols; j++)\n",
    "                    {{\n",
    "                        unsigned int idx_val = i * (numMatrices * Ncols)\n",
    "                                             + j * numMatrices\n",
    "                                             + tid;\n",
    "                        T val = inputMatrices[idx_val];\n",
    "                        sum += betav * rightHouseholderVectors[j] * val;\n",
    "                    }}\n",
    "                    wi[i] = sum;\n",
    "                }}\n",
    "\n",
    "                // 5) Build zi\n",
    "                T dot = static_cast<T>(0);\n",
    "                #pragma unroll\n",
    "                for (unsigned int i = 0; i < Ncols; i++)\n",
    "                    dot += xi[i] * rightHouseholderVectors[i];\n",
    "\n",
    "                #pragma unroll\n",
    "                for (unsigned int i = 0; i < Ncols; i++)\n",
    "                    zi[i] = xi[i] - betav * dot * rightHouseholderVectors[i];\n",
    "\n",
    "                // 6) Update matrix A^(tid)\n",
    "                #pragma unroll\n",
    "                for (unsigned int r = 0; r < Nrows; r++)\n",
    "                {{\n",
    "                    #pragma unroll\n",
    "                    for (unsigned int c = 0; c < Ncols; c++)\n",
    "                    {{\n",
    "                        unsigned int idx_old = r * (numMatrices * Ncols)\n",
    "                                             + c * numMatrices\n",
    "                                             + tid;\n",
    "                        T oldVal = inputMatrices[idx_old];\n",
    "                        T L = -leftHouseholderVectors[r] * zi[c];\n",
    "                        T R = -wi[r] * rightHouseholderVectors[c];\n",
    "                        inputMatrices[idx_old] = L + R + oldVal;\n",
    "                    }}\n",
    "                }}\n",
    "            }}\n",
    "            else\n",
    "            {{\n",
    "                // Ultima iterazione: solo left Householder\n",
    "                computeLeftHouseholderVectors(\n",
    "                    inputMatrices,\n",
    "                    leftHouseholderVectors,\n",
    "                    betau,\n",
    "                    tid,\n",
    "                    iterCounter);\n",
    "\n",
    "                // Build xi\n",
    "                #pragma unroll\n",
    "                for (unsigned int j = 0; j < Ncols; j++)\n",
    "                {{\n",
    "                    T sum = static_cast<T>(0);\n",
    "                    #pragma unroll\n",
    "                    for (unsigned int i = 0; i < Nrows; i++)\n",
    "                    {{\n",
    "                        unsigned int idx_val = i * (numMatrices * Ncols)\n",
    "                                             + j * numMatrices\n",
    "                                             + tid;\n",
    "                        T val = inputMatrices[idx_val];\n",
    "                        sum += leftHouseholderVectors[i] * val;\n",
    "                    }}\n",
    "                    xi[j] = sum;\n",
    "                }}\n",
    "\n",
    "                // Update matrix A^(tid)\n",
    "                #pragma unroll\n",
    "                for (unsigned int r = 0; r < Nrows; r++)\n",
    "                {{\n",
    "                    #pragma unroll\n",
    "                    for (unsigned int c = 0; c < Ncols; c++)\n",
    "                    {{\n",
    "                        unsigned int idx_old = r * (numMatrices * Ncols)\n",
    "                                             + c * numMatrices\n",
    "                                             + tid;\n",
    "                        T oldVal = inputMatrices[idx_old];\n",
    "                        inputMatrices[idx_old] = -betau * xi[c] * leftHouseholderVectors[r] + oldVal;\n",
    "                    }}\n",
    "                }}\n",
    "            }}\n",
    "        }}\n",
    "    }}\n",
    "    }}  // extern \"C\"\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiQ4zQgJ_ako"
   },
   "source": [
    "Extract diagonals - Assumes interlaced storage of matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6_eF5AB4_iLV"
   },
   "outputs": [],
   "source": [
    "def makeExtractDiagonalsKernel(dtype_str, numMatrices, Nrows, Ncols):\n",
    "    return f\"\"\"\n",
    "    #define T {dtype_str}\n",
    "    #define numMatrices {numMatrices}\n",
    "    #define Nrows {Nrows}\n",
    "    #define Ncols {Ncols}\n",
    "\n",
    "    extern \"C\" {{\n",
    "\n",
    "    __global__ void extractDiagonalsKernel(\n",
    "        const T * __restrict__ inputMatrices,\n",
    "        T * __restrict__ d,\n",
    "        T * __restrict__ e)\n",
    "    {{\n",
    "        int tid = threadIdx.x + blockDim.x * blockIdx.x;\n",
    "        if (tid >= numMatrices) return;\n",
    "\n",
    "        // estraiamo solo Nrows elementi per batch\n",
    "        #pragma unroll\n",
    "        for (unsigned int i = 0; i < Nrows - 1; i++) {{\n",
    "            // offset base: r = i, c = i\n",
    "            unsigned int base_diag = (i * Ncols + i) * numMatrices + tid;\n",
    "            d[ tid + i * numMatrices ] = inputMatrices[ base_diag ];\n",
    "\n",
    "            // offset super-diagonale: r = i, c = i+1\n",
    "            unsigned int base_sup  = (i * Ncols + (i + 1)) * numMatrices + tid;\n",
    "            e[ tid + i * numMatrices ] = inputMatrices[ base_sup ];\n",
    "        }}\n",
    "        // ultimo elemento diagonale (i = Nrows-1)\n",
    "        unsigned int i = Nrows - 1;\n",
    "        unsigned int base_last = (i * Ncols + i) * numMatrices + tid;\n",
    "        d[ tid + i * numMatrices ] = inputMatrices[ base_last ];\n",
    "        if (Ncols > Nrows) {{\n",
    "            unsigned int base_last = (i * Ncols + (i + 1)) * numMatrices + tid;\n",
    "            e[ tid + i * numMatrices ] = inputMatrices[ base_last ];\n",
    "        }}\n",
    "    }}\n",
    "    }}  // extern \"C\"\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c7fCSfye3Azb"
   },
   "source": [
    "Tridiagonal matrices computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "vpJHBs4t3EQ6"
   },
   "outputs": [],
   "source": [
    "def makeTridiagonalMatricesComputationKernel(dtype_str, numMatrices, Nrows, Ncols, LEN):\n",
    "    return f\"\"\"\n",
    "    #define T {dtype_str}\n",
    "    #define numMatrices {numMatrices}\n",
    "    #define Nrows {Nrows}\n",
    "    #define Ncols {Ncols}\n",
    "    #define LEN {LEN}\n",
    "\n",
    "    extern \"C\" {{\n",
    "\n",
    "      __global__ void tridiagFromBidiagKernel(\n",
    "          const T * __restrict__ d,    // [numMatrices*Nrows] interlacciato\n",
    "          const T * __restrict__ e,    // [numMatrices*LEN_BETA] interlacciato\n",
    "                T * __restrict__ alpha,// [numMatrices*Ncols] interlacciato\n",
    "                T * __restrict__ beta) // [numMatrices*(Ncols-1)] interlacciato\n",
    "      {{\n",
    "          const unsigned int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "          if (tid >= numMatrices) return;\n",
    "\n",
    "          alpha[0 * numMatrices + tid] = d[0 * numMatrices + tid] * d[0 * numMatrices + tid];\n",
    "          beta [0 * numMatrices + tid] = d[0 * numMatrices + tid] * e[0 * numMatrices + tid];\n",
    "\n",
    "          // i = 1 … Nrows-1  (diagonale)\n",
    "          #pragma unroll\n",
    "          for (unsigned int i = 1; i < Nrows; ++i) {{\n",
    "                T di = d[i * numMatrices + tid];\n",
    "                T ei_1 = e[(i - 1) * numMatrices + tid];\n",
    "                alpha[i * numMatrices + tid] = di * di + ei_1 * ei_1;\n",
    "          }}\n",
    "\n",
    "          // βᵢ = dᵢ * eᵢ  per i = 1 … LEN -1\n",
    "          #pragma unroll\n",
    "          for (unsigned int i = 1; i < LEN; ++i)\n",
    "                  beta[i*numMatrices + tid] = d[i*numMatrices + tid] * e[i*numMatrices + tid];\n",
    "\n",
    "          #pragma unroll\n",
    "          for (unsigned int i = Nrows; i < Ncols; ++i) {{\n",
    "          //for (unsigned int i = Nrows; i < Nrows + 1; ++i) {{\n",
    "              // qui non c'è più d[i] ma solo e[i-1]\n",
    "              if (i == Nrows) {{\n",
    "                T ei_1 = e[(i - 1) * numMatrices + tid];\n",
    "                alpha[i * numMatrices + tid] = ei_1 * ei_1;\n",
    "              }}\n",
    "              else\n",
    "                alpha[i * numMatrices + tid] = T(0);\n",
    "          }}\n",
    "      }}\n",
    "\n",
    "    }} // extern \"C\"\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZYeBsKgYPzPt"
   },
   "source": [
    "Pivots computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ktPRBSfLP0mp"
   },
   "outputs": [],
   "source": [
    "def makePivotsComputationKernel(dtype_str, numMatrices, Nrows):\n",
    "    eps_str = \"FLT_EPSILON\" if dtype_str==\"float\" else \"DBL_EPSILON\"\n",
    "    return f\"\"\"\n",
    "    #define T {dtype_str}\n",
    "    #define numMatrices {numMatrices}\n",
    "    #define Nrows {Nrows}\n",
    "\n",
    "    #include <cfloat>\n",
    "\n",
    "\n",
    "    // pick machineâ€“epsilon for our T\n",
    "    #define eps {eps_str}\n",
    "\n",
    "    // Macro absolute / max\n",
    "    //#define fabsT(x) ((T)fabs(x))\n",
    "    __device__ __forceinline__ float  fabsT(float  x) {{ return fabsf(x); }}\n",
    "    __device__ __forceinline__ double fabsT(double x) {{ return fabs (x); }}\n",
    "    //#define fmaxT(x,y) ((T)fmax(x,y))\n",
    "    __device__ __forceinline__ float  fmaxT(float  x, float y) {{ return fmaxf(x, y); }}\n",
    "    __device__ __forceinline__ double fmaxT(double x, double y) {{ return fmax (x, y); }}\n",
    "\n",
    "    extern \"C\" {{\n",
    "\n",
    "    __global__ void pivotsComputationKernel(\n",
    "        const T * __restrict__ b,   // [numMatrices * LEN]\n",
    "              T * __restrict__ piv) // [numMatrices]\n",
    "    {{\n",
    "      unsigned tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "      if (tid >= numMatrices) return;\n",
    "\n",
    "      const unsigned LEN = Nrows - 1;\n",
    "      // Trovo il massimo assoluto di b[0..LEN-1]\n",
    "      T localMax = fabsT(b[0*numMatrices + tid]);\n",
    "      #pragma unroll\n",
    "      for (unsigned i = 1; i < LEN; ++i) {{\n",
    "        T v = fabsT(b[i*numMatrices + tid]);\n",
    "        if (v > localMax) localMax = fmaxT(localMax, v);\n",
    "      }}\n",
    "\n",
    "      // pivot = Î²_max * eps * safety_factor (qui scelto 10)\n",
    "      piv[tid] = localMax * eps * (T)5;\n",
    "    }}\n",
    "\n",
    "    }} // extern \"C\"\n",
    "\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O65ua5z05UTM"
   },
   "source": [
    "Intervals computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "6uXpcfP85VsJ"
   },
   "outputs": [],
   "source": [
    "def makeComputeInitialIntervalsKernel(dtype_str, numMatrices, Nrows):\n",
    "    return f\"\"\"\n",
    "    #define T {dtype_str}\n",
    "    #define numMatrices {numMatrices}\n",
    "    #define Nrows {Nrows}\n",
    "\n",
    "    // file: initial_interval_kernel.cu\n",
    "    #include <cfloat>\n",
    "    #include <cmath>\n",
    "\n",
    "  __device__ __forceinline__ float  fabsT(float  x) {{ return fabsf(x); }}\n",
    "  __device__ __forceinline__ double fabsT(double x) {{ return fabs (x); }}\n",
    "\n",
    "    extern \"C\" {{\n",
    "\n",
    "    // ===============================================\n",
    "    // KERNEL: computeInitialIntervalKernel\n",
    "    // ===============================================\n",
    "    __global__ void computeInitialIntervalsKernel(\n",
    "        const T * __restrict__ d,       // [numMatrices * Nrows]\n",
    "        const T * __restrict__ b,       // [numMatrices * (Nrows-1)]  (se Nrows<=Ncols)\n",
    "    // output:\n",
    "        T * __restrict__ alpha_out,     // [numMatrices]\n",
    "        T * __restrict__ beta_out)      // [numMatrices]\n",
    "    {{\n",
    "        unsigned tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "        if (tid >= numMatrices) return;\n",
    "\n",
    "        // 1) prendo d[0], b[0]\n",
    "        T a = d[0*numMatrices + tid] - fabsT(b[0*numMatrices + tid]);\n",
    "        T be = fabsT(d[0*numMatrices + tid]) + fabsT(b[0*numMatrices + tid]);\n",
    "\n",
    "        // 2) ultimo elemento (i=Nrows-1) usa solo d[Nrows-1] e b[Nrows-2]\n",
    "        {{\n",
    "            unsigned i = Nrows-1;\n",
    "            T di   = d[i*numMatrices + tid];\n",
    "            T bi_1 = b[(i-1)*numMatrices + tid];\n",
    "            T t1   = di - fabsT(bi_1);\n",
    "            T t2   = fabsT(di) + fabsT(bi_1);\n",
    "            if (t1 < a)     a = t1;\n",
    "            if (t2 > be)    be = t2;\n",
    "        }}\n",
    "\n",
    "        // 3) generica i=1..Nrows-2\n",
    "        #pragma unroll\n",
    "        for (unsigned i = 1; i < Nrows-1; ++i) {{\n",
    "            T di    = d[i*numMatrices + tid];\n",
    "            T bi_1  = b[(i-1)*numMatrices + tid];\n",
    "            T bi    = b[i*numMatrices + tid];\n",
    "            T t1 = di - fabsT(bi_1) - fabsT(bi);\n",
    "            T t2 = di + fabsT(bi_1) + fabsT(bi);\n",
    "            if (t1 < a)  a = t1;\n",
    "            if (t2 > be) be = t2;\n",
    "        }}\n",
    "\n",
    "        alpha_out[tid] = a;\n",
    "        beta_out [tid] = be;\n",
    "    }}\n",
    "\n",
    "    }} // extern \"C\"\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJxvO5GRRZWn"
   },
   "source": [
    "Roots computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ApRyXPclc2LG"
   },
   "outputs": [],
   "source": [
    "def makeRootsComputationKernel(dtype_str, numMatrices, Nrows, Ncols):\n",
    "    return f\"\"\"\n",
    "    #define T {dtype_str}\n",
    "    #define numMatrices {numMatrices}\n",
    "    #define Nrows {Nrows}\n",
    "    #define Ncols {Ncols}\n",
    "\n",
    "  __device__ __forceinline__ float  fabsT(float  x) {{ return fabsf(x); }}\n",
    "  __device__ __forceinline__ double fabsT(double x) {{ return fabs (x); }}\n",
    "\n",
    "  extern \"C\" {{\n",
    "  __global__ void sturmBisectionKernel(\n",
    "      const T* __restrict__ dd,           // diagonale tridiagonale interlacciata [numMatrices*Ncols]\n",
    "      const T* __restrict__ bb,           // off‑diagonale tridiagonale interlacciata [numMatrices*(Ncols‑1)]\n",
    "      const T* __restrict__ pivots,       // pivot per ciascuna matrice [numMatrices]\n",
    "      const T* __restrict__ alpha,        // estremi inferiori [numMatrices]\n",
    "      const T* __restrict__ beta,         // estremi superiori [numMatrices]\n",
    "      T*       __restrict__ singularVals, // output valori singolari [numMatrices*Nrows]\n",
    "      const T  tol)                       // tolleranza bisezione\n",
    "  {{\n",
    "      int tidx = blockIdx.x * blockDim.x + threadIdx.x;  // indice di matrice\n",
    "      int tidy = blockIdx.y * blockDim.y + threadIdx.y;                            // indice di autovalore\n",
    "\n",
    "      if (tidx >= numMatrices || tidy >= Nrows) return;\n",
    "\n",
    "      // 1) recupero intervallo e pivot\n",
    "      T a     = alpha[tidx];\n",
    "      T b     = beta [tidx];\n",
    "      T piv   = pivots[tidx];\n",
    "      T c     = (T)0;\n",
    "      unsigned numChanges;\n",
    "\n",
    "      // 2) bisezione\n",
    "      T dist = fabsT(b - a);\n",
    "      T s    = fabsT(b) + fabsT(a);\n",
    "      while (dist > tol * s) {{\n",
    "          c = (a + b) * (T)0.5;\n",
    "          // conto il numero di cambi di segno della sequenza di Sturm\n",
    "          numChanges = 0;\n",
    "          // primo termine\n",
    "          T q = dd[   tidx] - c;\n",
    "          if (fabsT(q) <= piv) q = -piv;\n",
    "          if (q < (T)0) ++numChanges;\n",
    "          // ricorriamo alla formula ricorsiva per i=2..Ncols\n",
    "          #pragma unroll\n",
    "          for (unsigned i = 2; i <= Ncols; ++i) {{\n",
    "              T di   = dd[   (i-1)*numMatrices + tidx];\n",
    "              T bi_1 = bb[(i-2)*numMatrices + tidx];\n",
    "              q = (di - (bi_1*bi_1)/q) - c;\n",
    "              if (fabsT(q) <= piv) q = -piv;\n",
    "              if (q < (T)0) ++numChanges;\n",
    "          }}\n",
    "          // se i numeri di radici ≤ c superano Ncols - (eigIndex+1), restringo a [a,c], altrimenti [c,b]\n",
    "          if (numChanges > (Ncols - (tidy + 1))) b = c;\n",
    "          else                                  a = c;\n",
    "\n",
    "          dist = fabsT(b - a);\n",
    "          s    = fabsT(b) + fabsT(a);\n",
    "      }}\n",
    "\n",
    "      // 3) scrivo la radice (= valore singolare) nell’output\n",
    "      singularVals[ tidy + tidx * Nrows ] = sqrt( fabsT(c) );\n",
    "  }}\n",
    "  }} // extern \"C\"\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uz8iRDN6K4as"
   },
   "source": [
    "Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "SAJryuwDK6X2"
   },
   "outputs": [],
   "source": [
    "rearrangeKernel                 = SourceModule(makeRearrangeKernel(dtype_str, numMatrices, Nrows, Ncols), options=[\"--std=c++11\"], no_extern_c=True)\n",
    "bidiagonalizeKernel             = SourceModule(makeBidiagonalizeKernel(dtype_str, numMatrices, Nrows, Ncols), options=[\"--std=c++11\"], no_extern_c=True)\n",
    "extractDiagonalsKernel          = SourceModule(makeExtractDiagonalsKernel(dtype_str, numMatrices, Nrows, Ncols), options=[\"--std=c++11\"], no_extern_c=True)\n",
    "tridiagonalizeKernel            = SourceModule(makeTridiagonalMatricesComputationKernel(dtype_str, numMatrices, Nrows, Ncols, LEN), options=[\"--std=c++11\"], no_extern_c=True)\n",
    "pivotsComputationKernel         = SourceModule(makePivotsComputationKernel(dtype_str, numMatrices, Nrows), options=[\"--std=c++11\"], no_extern_c=True)\n",
    "computeInitialIntervalsKernel   = SourceModule(makeComputeInitialIntervalsKernel(dtype_str, numMatrices, Nrows), options=[\"--std=c++11\"], no_extern_c=True)\n",
    "rootsComputationKernel          = SourceModule(makeRootsComputationKernel(dtype_str, numMatrices, Nrows, Ncols), options=[\"--std=c++11\"], no_extern_c=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JoYwILz7LIpk"
   },
   "source": [
    "Recover the function pointers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "dLApZFm5LDrZ"
   },
   "outputs": [],
   "source": [
    "rearrange               = rearrangeKernel.get_function(\"rearrangeKernel\")\n",
    "bidiagonalize           = bidiagonalizeKernel.get_function(\"bidiagonalizeKernel\")\n",
    "extractDiagonals        = extractDiagonalsKernel.get_function(\"extractDiagonalsKernel\")\n",
    "tridiagonalize          = tridiagonalizeKernel.get_function(\"tridiagFromBidiagKernel\")\n",
    "pivotsComputation       = pivotsComputationKernel.get_function(\"pivotsComputationKernel\")\n",
    "intervalsComputation    = computeInitialIntervalsKernel.get_function(\"computeInitialIntervalsKernel\")\n",
    "rootsComputation        = rootsComputationKernel.get_function(\"sturmBisectionKernel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "nOsJlZ57V9y8"
   },
   "outputs": [],
   "source": [
    "def computeBatchedSVs(input_gpu_f, output_gpu_f, d_d, d_e, d_dd, d_bb, d_piv, d_alpha, d_beta, d_sv, numMatrices, Nrows, Ncols, workType):\n",
    "\n",
    "  bytes_per_elem      = np.dtype(workType).itemsize\n",
    "\n",
    "  blockSize           = 32\n",
    "  gridSize            = (int(numMatrices) + blockSize - 1) // blockSize  # = 1\n",
    "\n",
    "  #########################\n",
    "  # MATRIX REARRANGEMENTS #\n",
    "  #########################\n",
    "  rearrange(input_gpu_f, output_gpu_f, block=(blockSize,1,1), grid=(gridSize,1))\n",
    "  drv.Context.synchronize()\n",
    "  #print('rearrange done')\n",
    "\n",
    "  # --- Check tridiagonal extraction\n",
    "  '''h_output_gpu_f = np.zeros(numMatrices * Ncols * Nrows,     dtype=workType)\n",
    "  drv.memcpy_dtoh(h_output_gpu_f, output_gpu_f)\n",
    "\n",
    "  # Reshape and print for each matrix in batch\n",
    "  for k in range(numMatrices):\n",
    "      diag   = h_output_gpu_f   [k::numMatrices]      # picks [d0,d1,...] for batch k\n",
    "      print(f\"Rearrange batch {k}: matrix = {diag}\")'''\n",
    "\n",
    "  #####################\n",
    "  # BIDIAGONALIZATION #\n",
    "  #####################\n",
    "  bidiagonalize(output_gpu_f, maxIter, block=(blockSize,1,1), grid=(gridSize,1) )\n",
    "  drv.Context.synchronize()\n",
    "  #print('bidiagonalize done')\n",
    "\n",
    "  #####################\n",
    "  # EXTRACT DIAGONALS #\n",
    "  #####################\n",
    "  extractDiagonals(output_gpu_f, d_d, d_e, block=(blockSize,1,1), grid=(gridSize,1,1))\n",
    "  drv.Context.synchronize()\n",
    "  #print('extractDiagonals done')\n",
    "\n",
    "  ######################################################\n",
    "  # TRIDIAGONAL MATRICES COMPUTATIONS FROM BIDIAGONALS #\n",
    "  ######################################################\n",
    "  tridiagonalize(d_d, d_e, d_dd, d_bb, block=(blockSize,1,1), grid=(gridSize,1) )\n",
    "  drv.Context.synchronize()\n",
    "  #print('tridiagonalize done')\n",
    "\n",
    "  # --- Check tridiagonal extraction\n",
    "  '''h_dd = np.zeros(numMatrices * Ncols,     dtype=workType)\n",
    "  h_bb = np.zeros(numMatrices * (Ncols - 1), dtype=workType)\n",
    "  drv.memcpy_dtoh(h_dd, d_dd)\n",
    "  drv.memcpy_dtoh(h_bb, d_bb)\n",
    "\n",
    "  # Reshape and print for each matrix in batch\n",
    "  for k in range(numMatrices):\n",
    "      diag   = h_dd   [k::numMatrices]      # picks [d0,d1,...] for batch k\n",
    "      offdiag= h_bb   [k::numMatrices]      # picks [e0,e1,...] for batch k\n",
    "      print(f\"Batch {k}: dd = {diag}, ee = {offdiag}\")'''\n",
    "\n",
    "\n",
    "  ######################\n",
    "  # PIVOTS COMPUTATION #\n",
    "  ######################\n",
    "  pivotsComputation(d_bb, d_piv, block=(blockSize,1,1), grid=(gridSize,1) )\n",
    "  drv.Context.synchronize()\n",
    "  #print('pivotsComputation done')\n",
    "\n",
    "  #####################\n",
    "  # COMPUTE INTERVALS #\n",
    "  #####################\n",
    "  intervalsComputation(d_dd, d_bb, d_alpha, d_beta, block=(blockSize,1,1), grid=(gridSize,1))\n",
    "  drv.Context.synchronize()\n",
    "  #print('intervalsComputation done')\n",
    "\n",
    "  # --- Check interval computation\n",
    "  '''h_alpha = np.zeros(numMatrices,     dtype=workType)\n",
    "  h_beta = np.zeros(numMatrices, dtype=workType)\n",
    "  drv.memcpy_dtoh(h_alpha, d_alpha)\n",
    "  drv.memcpy_dtoh(h_beta, d_beta)\n",
    "\n",
    "  # Reshape and print for each matrix in batch\n",
    "  for k in range(numMatrices):\n",
    "      diag   = h_alpha   [k::numMatrices]      # picks [d0,d1,...] for batch k\n",
    "      offdiag= h_beta   [k::numMatrices]      # picks [e0,e1,...] for batch k\n",
    "      print(f\"Compute intervals batch {k}: alpha = {diag}, beta = {offdiag}\")'''\n",
    "\n",
    "  #####################\n",
    "  # ROOTS COMPUTATION #\n",
    "  #####################\n",
    "\n",
    "  bx = 16\n",
    "  by = 16\n",
    "  gx = (int(numMatrices) + bx - 1) // bx\n",
    "  gy = (int(Nrows) + by - 1) // by\n",
    "\n",
    "  rootsComputation(d_dd, d_bb, d_piv, d_alpha, d_beta, d_sv, sturmTol, block=(bx, by, 1), grid=(gx, gy, 1))\n",
    "  drv.Context.synchronize()\n",
    "  #print('rootsComputation done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pw0luXlLYsnj"
   },
   "source": [
    "Standard random matrix generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "pkukY00JPK-i",
    "outputId": "42bfbfa1-9597-48ac-f66c-cae1e61fb193"
   },
   "outputs": [],
   "source": [
    "h_matrices = np.asfortranarray(\n",
    "    np.random.randn(numMatrices, Nrows, Ncols).astype(workType)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rL6fcT0xYzGC"
   },
   "source": [
    "Random matrix generation according to a prescribed pattern of the singular values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "PmMEdK55PM4h",
    "outputId": "90647d84-70b1-43e0-eb44-04464588e378"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"m = Nrows = Ncols\\n\\nbase_kappas = np.array([2,4,6,8,10])\\n# e.g. repeat each value 10 times\\nkappas = np.repeat(base_kappas, numMatrices // len(base_kappas))\\n# if numMatrices isn't a multiple of 5, append a few more to reach it:\\nwhile kappas.size < numMatrices:\\n    kappas = np.concatenate([kappas, base_kappas[: (numMatrices - kappas.size)]])\\n# preallocate\\nbatch = np.empty((numMatrices, m, m), dtype=workType, order='F')\\n\\nfor j, κ in enumerate(kappas):\\n    # 1) geometric spectrum from cond=1 to cond=10^κ\\n    s = 10**np.linspace(0, κ, m)\\n\\n    # 2) random orthonormal U, V via QR\\n    #    (np.linalg.qr returns Q,R; we only need Q)\\n    U, _ = np.linalg.qr(np.random.randn(m, m))\\n    V, _ = np.linalg.qr(np.random.randn(m, m))\\n\\n    # 3) assemble\\n    A = U @ np.diag(s) @ V.T\\n\\n    # store (astype and Fortran‐order)\\n    batch[j, :, :] = A.astype(workType, copy=False)\\n\\n# ensure entire batch is Fortran‐contiguous\\nh_matrices = np.asfortranarray(batch)\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''m = Nrows = Ncols\n",
    "\n",
    "base_kappas = np.array([2,4,6,8,10])\n",
    "# e.g. repeat each value 10 times\n",
    "kappas = np.repeat(base_kappas, numMatrices // len(base_kappas))\n",
    "# if numMatrices isn't a multiple of 5, append a few more to reach it:\n",
    "while kappas.size < numMatrices:\n",
    "    kappas = np.concatenate([kappas, base_kappas[: (numMatrices - kappas.size)]])\n",
    "# preallocate\n",
    "batch = np.empty((numMatrices, m, m), dtype=workType, order='F')\n",
    "\n",
    "for j, κ in enumerate(kappas):\n",
    "    # 1) geometric spectrum from cond=1 to cond=10^κ\n",
    "    s = 10**np.linspace(0, κ, m)\n",
    "\n",
    "    # 2) random orthonormal U, V via QR\n",
    "    #    (np.linalg.qr returns Q,R; we only need Q)\n",
    "    U, _ = np.linalg.qr(np.random.randn(m, m))\n",
    "    V, _ = np.linalg.qr(np.random.randn(m, m))\n",
    "\n",
    "    # 3) assemble\n",
    "    A = U @ np.diag(s) @ V.T\n",
    "\n",
    "    # store (astype and Fortran‐order)\n",
    "    batch[j, :, :] = A.astype(workType, copy=False)\n",
    "\n",
    "# ensure entire batch is Fortran‐contiguous\n",
    "h_matrices = np.asfortranarray(batch)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7WZzfjpsZ4Wi"
   },
   "source": [
    "Random matrix generation by perturbing a fixed random matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "B_Pyy6dTaIRi"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from numpy.linalg import norm\\n\\n# Parameters\\nm = Nrows = Ncols            # assume square\\n\\nepsilons = np.array([1e-8, 1e-7, 1e-6, 1e-5], dtype=workType)\\n\\n# 1) Build your “clean” base matrix A\\n#    For example: a random Gaussian, or reuse one from your Experiment\\u202f2\\nA = np.random.randn(m, m).astype(workType, copy=False)\\n\\n# Precompute its 2‐norm\\nA_norm2 = norm(A, 2)\\n\\n# 2) Allocate batch container [J × m × m] in Fortran order\\nJ = epsilons.size\\nbatch = np.empty((J, m, m), dtype=workType, order='F')\\n\\n# 3) Fill in each noisy realization\\nfor j, eps in enumerate(epsilons):\\n    # draw noise matrix G ~ N(0,1)\\n    G = np.random.randn(m, m).astype(workType, copy=False)\\n    # form noisy matrix\\n    batch[j, :, :] = (A + eps * A_norm2 * G)\\n\\n# 4) Ensure Fortran‐contiguity (batch was created as such)\\nh_matrices = np.asfortranarray(batch)\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from numpy.linalg import norm\n",
    "\n",
    "# Parameters\n",
    "m = Nrows = Ncols            # assume square\n",
    "\n",
    "epsilons = np.array([1e-8, 1e-7, 1e-6, 1e-5], dtype=workType)\n",
    "\n",
    "# 1) Build your “clean” base matrix A\n",
    "#    For example: a random Gaussian, or reuse one from your Experiment 2\n",
    "A = np.random.randn(m, m).astype(workType, copy=False)\n",
    "\n",
    "# Precompute its 2‐norm\n",
    "A_norm2 = norm(A, 2)\n",
    "\n",
    "# 2) Allocate batch container [J × m × m] in Fortran order\n",
    "J = epsilons.size\n",
    "batch = np.empty((J, m, m), dtype=workType, order='F')\n",
    "\n",
    "# 3) Fill in each noisy realization\n",
    "for j, eps in enumerate(epsilons):\n",
    "    # draw noise matrix G ~ N(0,1)\n",
    "    G = np.random.randn(m, m).astype(workType, copy=False)\n",
    "    # form noisy matrix\n",
    "    batch[j, :, :] = (A + eps * A_norm2 * G)\n",
    "\n",
    "# 4) Ensure Fortran‐contiguity (batch was created as such)\n",
    "h_matrices = np.asfortranarray(batch)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "hjWKAqfGizN6"
   },
   "outputs": [],
   "source": [
    "# Concatenate the random matrices\n",
    "h_input_col = np.zeros(numMatrices * Nrows * Ncols, dtype=workType)\n",
    "for k in range(numMatrices):\n",
    "    h_input_col[k*(Nrows*Ncols):(k+1)*(Nrows*Ncols)] = h_matrices[k].flatten(order='F')\n",
    "\n",
    "input_gpu_f       = drv.mem_alloc(h_input_col.nbytes)\n",
    "drv.memcpy_htod(input_gpu_f, h_input_col)\n",
    "\n",
    "bytes_per_elem      = np.dtype(workType).itemsize\n",
    "d_sv                = drv.mem_alloc(int(numMatrices * Nrows * bytes_per_elem))\n",
    "\n",
    "h_sv                = np.zeros(numMatrices * Nrows, dtype=workType);\n",
    "\n",
    "output_gpu_f        = drv.mem_alloc(int(numMatrices * Ncols * Nrows * bytes_per_elem))\n",
    "\n",
    "# Allocate GPU space for diagonals\n",
    "size_d              = numMatrices * Nrows * bytes_per_elem\n",
    "size_e              = numMatrices * LEN   * bytes_per_elem\n",
    "d_d                 = drv.mem_alloc(int(size_d))                                  # --- Main diagonal of the bidiagonal matrix\n",
    "d_e                 = drv.mem_alloc(int(size_e))                                  # --- Upper diagonal of the bidiagonal matrix\n",
    "\n",
    "size_dd             = numMatrices * Ncols * bytes_per_elem\n",
    "size_bb             = numMatrices * (Ncols - 1) * bytes_per_elem\n",
    "d_dd                = drv.mem_alloc(int(size_dd))                                 # --- alpha tridiagonal\n",
    "d_bb                = drv.mem_alloc(int(size_bb))                                 # --- beta tridiagonal\n",
    "\n",
    "size_piv            = numMatrices * bytes_per_elem\n",
    "d_piv               = drv.mem_alloc(int(size_piv))\n",
    "\n",
    "size_alpha_beta     = numMatrices * bytes_per_elem\n",
    "d_alpha             = drv.mem_alloc(int(size_alpha_beta))\n",
    "d_beta              = drv.mem_alloc(int(size_alpha_beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yI4BSLW067lS"
   },
   "source": [
    "Warmup execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wYTVgRZV69If",
    "outputId": "cc6853e2-7d2f-4acf-fc3d-8a8041e3fd22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size numMatrices = 8388608 → avg error: 4.386e-12\n"
     ]
    }
   ],
   "source": [
    "computeBatchedSVs(input_gpu_f, output_gpu_f, d_d, d_e, d_dd, d_bb, d_piv, d_alpha, d_beta, d_sv, numMatrices, Nrows, Ncols, workType)\n",
    "drv.memcpy_dtoh(h_sv, d_sv)\n",
    "sv_matrix         = h_sv.reshape((numMatrices, Nrows), order='C')\n",
    "\n",
    "err_avg = 0.\n",
    "for k in range(numMatrices):\n",
    "    A_k = h_matrices[k]\n",
    "    s = np.linalg.svd(A_k, compute_uv=False)\n",
    "    # Compare only the first min(Nrows, Ncols) singular values\n",
    "    err = 100. * np.linalg.norm(s - sv_matrix[k][:min(Nrows, Ncols)]) / np.linalg.norm(s)\n",
    "    #print(err)\n",
    "    #print(s)\n",
    "    err_avg += err\n",
    "\n",
    "err_avg = err_avg / numMatrices\n",
    "    #print(f\"Matrix {k}:  numpy SVD singular values = {s}\")\n",
    "    #print(f\"Matrix {k}: \", sv_matrix[k])\n",
    "\n",
    "print(f\"Batch size numMatrices = {numMatrices} → avg error: {err_avg:.3e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90H5goyZa4xH"
   },
   "source": [
    "True execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bWr_sBt5a6E3",
    "outputId": "f43474e6-6719-48df-ffd8-c2c7a05b3193"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size numMatrices = 8388608 → elapsed: 0.422659 s, avg error: 4.386e-12\n"
     ]
    }
   ],
   "source": [
    "# Prepare CUDA events for timing\n",
    "start = drv.Event()\n",
    "end   = drv.Event()\n",
    "# Record start\n",
    "start.record()\n",
    "computeBatchedSVs(input_gpu_f, output_gpu_f, d_d, d_e, d_dd, d_bb, d_alpha, d_beta, d_piv, d_sv, numMatrices, Nrows, Ncols, workType)\n",
    "end.record()\n",
    "end.synchronize()\n",
    "elapsed_ms = start.time_till(end)\n",
    "\n",
    "#h_sv              = np.zeros(numMatrices * Nrows, dtype=workType)\n",
    "drv.memcpy_dtoh(h_sv, d_sv)\n",
    "sv_matrix         = h_sv.reshape((numMatrices, Nrows), order='C')\n",
    "\n",
    "#print(\"\\n=== Comparison with NumPy SVD ===\")\n",
    "err_avg = 0.\n",
    "for k in range(numMatrices):\n",
    "    A_k = h_matrices[k]\n",
    "    s = np.linalg.svd(A_k, compute_uv=False)\n",
    "    #err = 100. * np.linalg.norm(s - sv_matrix[k]) / np.linalg.norm(s)\n",
    "    err_avg += 100. * np.linalg.norm(s - sv_matrix[k][:min(Nrows, Ncols)]) / np.linalg.norm(s)\n",
    "    #print(f\"Matrix {k}: \", sv_matrix[k])\n",
    "    #print(f\"Matrix {k}:  numpy SVD singular values = {s}\")\n",
    "\n",
    "err_avg = err_avg / numMatrices\n",
    "\n",
    "print(f\"Batch size numMatrices = {numMatrices} → elapsed: {elapsed_ms / 1000:.6f} s, avg error: {err_avg:.3e}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
